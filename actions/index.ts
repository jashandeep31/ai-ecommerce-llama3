// "use server";

// import { ChatOllama } from "@langchain/ollama";

// const llm = new ChatOllama({
//   baseUrl: "http://localhost:11434",
//   model: "llama3.2",
// });

// export async function handleQueryOllama() {
//   const res = await llm.invoke("What is the size of your camera?");
//   console.log(res);
// }
